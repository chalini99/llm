{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NmKLtJHBqgg",
        "outputId": "a768048c-835d-41b9-ebf7-1d892d27b154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello World\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = [1,2,3,4,5]\n",
        "print(arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gisRSNlFB1ho",
        "outputId": "c54564b8-7183-4442-d13c-6e3bc8ef98a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2, 3],\n",
        "              [4, 5, 6]])\n",
        "\n",
        "B = np.array([[7, 8],\n",
        "              [9, 10],\n",
        "              [11, 12]])\n",
        "\n",
        "result = np.dot(A, B)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0d1OrVREpyy",
        "outputId": "9fe862e2-4457-4e83-e4fb-ba6e8306afe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 58  64]\n",
            " [139 154]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list = [10,20,30,40,50]\n",
        "print(list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U0bBvT6ObJY",
        "outputId": "3e0b18c1-9250-45da-dd4b-c6ba118b7f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10, 20, 30, 40, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install NLTK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke5WDeLkPCfY",
        "outputId": "a69b62b4-26f6-4086-978c-35e738493c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from NLTK) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from NLTK) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from NLTK) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from NLTK) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Large Languge Models are transforming Natural Language Processing\"\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ciVl8pAQZ4e",
        "outputId": "56015184-195c-44ce-fa92-7427d4172a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Large', 'Languge', 'Models', 'are', 'transforming', 'Natural', 'Language', 'Processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Character level tokenization\n",
        "text = \"Large Languge Models are transforming Natural Language Processing\"\n",
        "print(\"\\n=========== CHARACTER-LEVEL TOKENIZATION ========\")\n",
        "char_tokens = list(text)\n",
        "print(char_tokens[:50], \"...\")"
      ],
      "metadata": {
        "id": "b6fvsCzNQrE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d681da-7364-47fa-f684-168ee6fe0d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=========== CHARACTER-LEVEL TOKENIZATION ========\n",
            "['L', 'a', 'r', 'g', 'e', ' ', 'L', 'a', 'n', 'g', 'u', 'g', 'e', ' ', 'M', 'o', 'd', 'e', 'l', 's', ' ', 'a', 'r', 'e', ' ', 't', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'i', 'n', 'g', ' ', 'N', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'L', 'a', 'n', 'g'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "print(\"\\n=========== SUBWORD TOKENIZATION (BPE) ==============\")\n",
        "# 'text' is already defined from a previous cell\n",
        "corpus = [text]\n",
        "bpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bpe_tokenizer.train_from_iterator(\n",
        "    corpus,\n",
        "    vocab_size=100,\n",
        "    min_frequency=1\n",
        ")\n",
        "bpe_tokens = bpe_tokenizer.encode(text).tokens\n",
        "print(bpe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wW0Kf58VI_q",
        "outputId": "b6ea96c1-65de-4ea3-89c6-d7308bd8ab33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=========== SUBWORD TOKENIZATION (BPE) ==============\n",
            "['L', 'a', 'r', 'g', 'e', 'Ġ', 'L', 'a', 'n', 'g', 'u', 'g', 'e', 'Ġ', 'M', 'o', 'd', 'e', 'l', 's', 'Ġ', 'a', 'r', 'e', 'Ġ', 't', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'i', 'n', 'g', 'Ġ', 'N', 'a', 't', 'u', 'r', 'a', 'l', 'Ġ', 'L', 'a', 'n', 'g', 'u', 'a', 'g', 'e', 'Ġ', 'P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print(\"\\n============ WORDPIECE TOKENIZATION (BERT) =============\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_tokens = bert_tokenizer.tokenize(text)\n",
        "print(bert_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smX9T9RSVfJL",
        "outputId": "8de77754-a1b4-4148-d98a-177792f0ae37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============ WORDPIECE TOKENIZATION (BERT) =============\n",
            "['large', 'lang', '##uge', 'models', 'are', 'transforming', 'natural', 'language', 'processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyCxEgDGWLnR",
        "outputId": "330fb8d1-f10b-466c-9782-ff8cd0fe483e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentences = [\n",
        "    word_tokenize(sentence.lower())\n",
        "    for sentence in corpus\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb-YUeF8XeC5",
        "outputId": "f48c203f-c896-460a-b54e-fa8b0546f7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1\n",
        ")"
      ],
      "metadata": {
        "id": "XNpW6R8oYGoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word2Vec embedding (first 10 values) for 'language':\")\n",
        "print(w2v_model.wv[\"language\"][:10], \"....\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOKrbUW-YyW8",
        "outputId": "25681ab2-c7c2-4cfe-828d-a9fa95fba652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec embedding (first 10 values) for 'language':\n",
            "[-0.00861969  0.00366574  0.00518988  0.00574194  0.00746692 -0.00616768\n",
            "  0.00110561  0.00604728 -0.00284005 -0.00617352] ....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "36CAm3JVa5xj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}